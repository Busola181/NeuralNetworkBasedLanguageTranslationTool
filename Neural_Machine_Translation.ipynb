{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOj33MODYm8lCY/cZ31mYGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Busola181/NeuralNetworkBasedLanguageTranslationTool/blob/main/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "♈IMPORTING DRIVE"
      ],
      "metadata": {
        "id": "w_GzPe46fc-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP_fNAGCI6vl",
        "outputId": "d43856f7-2b8e-479d-d6b0-0f7ab9c36875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**♒"
      ],
      "metadata": {
        "id": "2E1foPSNftu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY_PATH =\"/content/drive/MyDrive/kaggle.json\"\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp $API_KEY_PATH ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d devicharith/language-translation-englishfrench\n",
        "\n",
        "RESOURCES_PATH=\"/content/language-translation-englishfrench.zip\"\n",
        "!cp $RESOURCES_PATH.\n",
        "!unzip /content/language-translation-englishfrench.zip -d /content/"
      ],
      "metadata": {
        "id": "7dX_uke0I2RI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029add8a-54c4-4813-e96f-61ef4c5ac7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench\n",
            "License(s): CC0-1.0\n",
            "Downloading language-translation-englishfrench.zip to /content\n",
            " 85% 3.00M/3.51M [00:00<00:00, 5.46MB/s]\n",
            "100% 3.51M/3.51M [00:00<00:00, 5.28MB/s]\n",
            "cp: missing destination file operand after '.'\n",
            "Try 'cp --help' for more information.\n",
            "Archive:  /content/language-translation-englishfrench.zip\n",
            "  inflating: /content/eng_-french.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMfGRfUIHuzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a92aef12-08be-4afb-bec4-03c3265a596f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /usr/local/share/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "import math\n",
        "import nltk\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.data.path.append('/usr/local/share/nltk_data')\n",
        "nltk.download('wordnet', download_dir='/usr/local/share/nltk_data')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/eng_-french.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "JP8hXXQHX8JH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b7411c-16ee-425c-9a12-3e3a04cbe372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['English words/sentences', 'French words/sentences'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = set(filter(lambda char: char in string.punctuation, dataset_path))\n",
        "print(punctuations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXJXMYDPPlcv",
        "outputId": "9239cccd-e196-4fc4-a558-0d551f2dd7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'_', '.', '/', '-'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(dataset_path, column_path):\n",
        "    text = df[column_path].str.cat(sep = '')\n",
        "    words = text.split()\n",
        "    print(words[:100])\n",
        "\n",
        "load_doc(dataset_path, 'English words/sentences')\n"
      ],
      "metadata": {
        "id": "91ZRL81fC1_6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb0450f-7315-484b-c0dc-885b9db182d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi.Run!Run!Who?Wow!Fire!Help!Jump.Stop!Stop!Stop!Wait!Wait!Go', 'on.Go', 'on.Go', 'on.Hello!Hello!I', 'see.I', 'try.I', 'won!I', 'won!I', 'won.Oh', 'no!Attack!Attack!Cheers!Cheers!Cheers!Cheers!Get', 'up.Go', 'now.Go', 'now.Go', 'now.Got', 'it!Got', 'it!Got', 'it?Got', 'it?Got', 'it?Hop', 'in.Hop', 'in.Hug', 'me.Hug', 'me.I', 'fell.I', 'fell.I', 'know.I', 'left.I', 'left.I', 'lied.I', 'lost.I', \"paid.I'm\", \"19.I'm\", \"OK.I'm\", 'OK.Listen.No', 'way!No', 'way!No', 'way!No', 'way!No', 'way!No', 'way!No', 'way!No', 'way!No', 'way!Really?Really?Really?Thanks.We', 'try.We', 'won.We', 'won.We', 'won.We', 'won.Ask', 'Tom.Awesome!Be', 'calm.Be', 'calm.Be', 'calm.Be', 'cool.Be', 'fair.Be', 'fair.Be', 'fair.Be', 'fair.Be', 'fair.Be', 'fair.Be', 'kind.Be', 'nice.Be', 'nice.Be', 'nice.Be', 'nice.Be', 'nice.Be', 'nice.Beat', 'it.Call', 'me.Call', 'me.Call', 'us.Call', 'us.Come', 'in.Come', 'in.Come', 'in.Come', 'in.Come', 'on!Come', 'on.Come', 'on.Come', 'on.Drop', 'it!Drop', 'it!Drop', 'it!Drop', 'it!Get', 'Tom.Get', 'out!Get', 'out!Get', 'out!Get', 'out.Get', 'out.Go', 'away!Go', 'away!Go', 'away.Go', 'away.Go', 'away.Go', 'away.Go', 'away.Go', 'away.Go', 'away.Go', 'away.Go', 'home.Go']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(dataset_path):\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    return df\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = normalize('NFD', text).encode('ascii', 'ignore').decode('UTF-8')\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[_./-]\", \" \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def tokenize_pairs(df):\n",
        "    df['English tokens'] = df['English words/sentences'].apply(preprocess_text)\n",
        "    df['French tokens'] = df['French words/sentences'].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "df = load_doc(dataset_path)\n",
        "\n",
        "df_tokenized = tokenize_pairs(df)\n",
        "\n",
        "print(df_tokenized[['English tokens', 'French tokens']].head(2))\n"
      ],
      "metadata": {
        "id": "Qb7CwqxbKjbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90a6603-8aa7-44ee-bf70-749bbbbaa6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  English tokens French tokens\n",
            "0           [hi]       [salut]\n",
            "1          [run]       [cours]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab(english_tokens, french_tokens, min_freq=1):\n",
        "\n",
        "    english_flat = [word for sentence in english_tokens for word in sentence]\n",
        "    french_flat = [word for sentence in french_tokens for word in sentence]\n",
        "\n",
        "    english_freq = Counter(english_flat)\n",
        "    french_freq = Counter(french_flat)\n",
        "\n",
        "    english_vocab = {word: idx + 4 for idx, (word, _) in enumerate(english_freq.items()) if english_freq[word] >= min_freq}\n",
        "    french_vocab = {word: idx + 4 for idx, (word, _) in enumerate(french_freq.items()) if french_freq[word] >= min_freq}\n",
        "\n",
        "    special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']\n",
        "    for token in special_tokens:\n",
        "        english_vocab[token] = len(english_vocab) + 1\n",
        "        french_vocab[token] = len(french_vocab) + 1\n",
        "\n",
        "    return english_vocab, french_vocab\n",
        "\n",
        "def text_to_tokens(text, vocab):\n",
        "    return [vocab.get(word, vocab['<UNK>']) for word in text]\n",
        "\n",
        "def pad_sequences(sequences, max_len, pad_token_idx):\n",
        "    return [seq + [pad_token_idx] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n",
        "\n",
        "english_tokens = df_tokenized['English tokens'].tolist()\n",
        "french_tokens = df_tokenized['French tokens'].tolist()\n",
        "\n",
        "english_vocab, french_vocab = create_vocab(english_tokens, french_tokens)\n",
        "\n",
        "\n",
        "def text_to_tokens(text, vocab):\n",
        "    return [vocab.get(word, vocab['<UNK>']) for word in text]\n",
        "\n",
        "def pad_sequences(sequences, max_len, pad_token_idx):\n",
        "    return [seq + [pad_token_idx] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n",
        "\n",
        "df_tokenized = tokenize_pairs(df)\n",
        "english_tokens = df_tokenized['English tokens'].tolist()\n",
        "french_tokens = df_tokenized['French tokens'].tolist()\n",
        "\n",
        "english_vocab, french_vocab = create_vocab(english_tokens, french_tokens)\n",
        "\n",
        "sample_english = df_tokenized['English tokens'].iloc[0]\n",
        "sample_french = df_tokenized['French tokens'].iloc[0]\n",
        "english_tokenized = text_to_tokens(sample_english, english_vocab)\n",
        "french_tokenized = text_to_tokens(sample_french, french_vocab)\n",
        "\n",
        "eng_input_tensor = torch.tensor(english_tokenized, dtype = torch.long)\n",
        "fren_target_tensor = torch.tensor(french_tokenized, dtype = torch.long)\n",
        "\n",
        "print(eng_input_tensor.shape)\n",
        "print(fren_target_tensor.shape)\n",
        "\n",
        "# eng_input_tensor = sorted(set)\n"
      ],
      "metadata": {
        "id": "00QdbOjxS7qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99c9a09-9a3a-42bc-93f3-f64941fe1cb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(english_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGNs9VwumhRc",
        "outputId": "099738f3-6e0c-415a-fc66-bb1f018d89e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hi': 4, 'run': 5, 'who': 6, 
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(french_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjdCteSrmqdc",
        "outputId": "30dfffab-8ac4-4374-f626-59f2b1ea1581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "mod_vec_dim = 512\n",
        "vocab_size = len(english_vocab)\n",
        "fre_vocab_size = len(french_vocab)\n",
        "src_seq_len = 100\n",
        "tgt_seq_len = 100\n",
        "embed_size = 512\n",
        "num_heads = 8\n",
        "hidden_dim = 2048"
      ],
      "metadata": {
        "id": "nvTH357jVuAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, english_tokens, french_tokens, english_vocab, french_vocab):\n",
        "        self.english_tokens = english_tokens\n",
        "        self.french_tokens = french_tokens\n",
        "        self.english_vocab = english_vocab\n",
        "        self.french_vocab = french_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        english_sentence = self.english_tokens[idx]\n",
        "        french_sentence = self.french_tokens[idx]\n",
        "\n",
        "        english_indices = text_to_tokens(english_sentence, self.english_vocab)\n",
        "        french_indices = text_to_tokens(french_sentence, self.french_vocab)\n",
        "\n",
        "        return torch.tensor(english_indices), torch.tensor(french_indices)\n",
        "\n",
        "    def collate_fn(batch, pad_token_idx=0):\n",
        "        english_batch, french_batch = zip(*batch)\n",
        "\n",
        "        english_max_len = max([len(sentence) for sentence in english_batch])\n",
        "        french_max_len = max([len(sentence) for sentence in french_batch])\n",
        "\n",
        "        english_batch_padded = pad_sequences(english_batch, english_max_len, pad_token_idx)\n",
        "        french_batch_padded = pad_sequences(french_batch, french_max_len, pad_token_idx)\n",
        "\n",
        "        english_batch_padded = torch.tensor(english_batch_padded)\n",
        "        french_batch_padded = torch.tensor(french_batch_padded)\n",
        "\n",
        "        return english_batch_padded, french_batch_padded\n",
        "\n",
        "    english_tokens = df_tokenized['English tokens'].tolist()\n",
        "    french_tokens = df_tokenized['French tokens'].tolist()\n",
        "\n",
        "    dataset = TranslationDataset(english_tokens, french_tokens, english_vocab, french_vocab)\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    for batch in dataloader:\n",
        "        english_batch, french_batch = batch\n",
        "        print(english_batch.shape)\n",
        "        print(french_batch.shape)   # Shape: [batch_size, max_len_french]\n",
        "        break\n"
      ],
      "metadata": {
        "id": "KtC2fMtom2Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class input_Embedding(nn.Module):\n",
        "    def __init__(self, mod_vec_dim, vocab_size):\n",
        "        super().__init__()\n",
        "        self.mod_vec_dim = mod_vec_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed = nn.Embedding(vocab_size, mod_vec_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x) * math.sqrt(self.mod_vec_dim)\n",
        "\n",
        "class position_encoding(nn.Module):\n",
        "    def __init__(self, mod_vec_dim, seq_len):\n",
        "        super().__init__()\n",
        "        self.mod_vec_dim = mod_vec_dim\n",
        "        self.seq_len = seq_len\n",
        "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, mod_vec_dim, 2).float() * -math.log(10000)/ mod_vec_dim)\n",
        "        pe = torch.zeros(mod_vec_dim, seq_len)\n",
        "        pe[:, 0:2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2]= torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer = ('pe', pe.unsqueeze(0))\n",
        "\n",
        "        self.dropout = nn.Dropout(p= 0.2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return  x + (self.pe[:, :x.size(1), :]).requires_grad_(False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "# embed_dim == model_vec_dim\n",
        "# For single head\n",
        "class self_attention(nn.Module):\n",
        "    def __init__(self, query, key, value, head_size):\n",
        "        super(self_attention, self).__init__()\n",
        "        self.query = nn.Linear(embed_dim, head_size bias =False)\n",
        "        self.key = nn.Linear(embed_dim, head_size, bias =False)\n",
        "        self.value = nn.Linear(embed_dim, head_size, bias =False)\n",
        "        self.head_size = head_size\n",
        "        self.sofmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1))/ math.sqrt(self.head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "\n",
        "        out = torch.matmul(attention_probs, value)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# For Multi-head\n",
        "\n",
        "class Multihead_Attention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, mask):\n",
        "        super(Multihead_Attention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.head_size = head_size\n",
        "        self.num_head = num_head\n",
        "        self.head_dim = embed_size // no_of_head\n",
        "\n",
        "        assert self.head_dim * num_head == embed_size\n",
        "\n",
        "        self.w_q = nn.Linear(embed_size, embed_size)\n",
        "        self.w_k = nn.Linear(embed_size, embed_size)\n",
        "        self.w_v = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, embed_Size)\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        N = query.shape[0]\n",
        "        seq_len = query.shape[1]\n",
        "\n",
        "        Q = self.w_q(query).view(N, seq_len, self.num_head, self.head_dim).transpose(1, 2)\n",
        "        K = self.w_k(key).view(N, seq_len, self.num_head, self.head_dim).transpose(1, 2)\n",
        "        V = self.w_v(value).view(N, seq_len, self.num_head, self.head_dim).transpose(1, 2)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)/ math.sqrt(self.head_dim))\n",
        "\n",
        "        if masked is not None:\n",
        "            attention_scores = attention_scores.masked_filled(mask == 0, float('-1e20'))\n",
        "\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        out = torch.matmul(attention_probs, V)\n",
        "\n",
        "        out = out.transpose(1, 2).contigous().view(N, seq_len, self.embed_size)\n",
        "        out = self.fc_out(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "KBZneks2qaMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n"
      ],
      "metadata": {
        "id": "RMHJoc8lo02u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in data_loader:\n",
        "        src = src.permute(1, 0)\n",
        "        tgt_input = tgt[:, :-1].permute(1, 0)\n",
        "        tgt_output = tgt[:, 1:].permute(1, 0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, tgt_input)\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        tgt_output = tgt_output.contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader)}\")\n"
      ],
      "metadata": {
        "id": "MJNUDAJEHESL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
